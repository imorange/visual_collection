define({ entries : {
    "10160491": {
        "abstract": "Grasping an object when it is in an ungraspable pose is a challenging task, such as books or other large flat objects placed horizontally on a table. Inspired by human manipulation, we address this problem by pushing the object to the edge of the table and then grasping it from the hanging part. In this paper, we develop a model-free Deep Reinforcement Learning framework to synergize pushing and grasping actions. We first pre-train a Variational Autoencoder to extract high-dimensional features of input scenario images. One Proximal Policy Optimization algorithm with the common reward and sharing layers of Actor-Critic is employed to learn both pushing and grasping actions with high data efficiency. Experiments show that our one network policy can converge 2.5 times faster than the policy using two parallel networks. Moreover, the experiments on unseen objects show that our policy can generalize to the challenging case of objects with curved surfaces and off-center irregularly shaped objects. Lastly, our policy can be transferred to a real robot without fine-tuning by using CycleGAN for domain adaption and outperforms the push-to-wall baseline.",
        "author": "Zhang, Hao and Liang, Hongzhuo and Cong, Lin and Lyu, Jianzhi and Zeng, Long and Feng, Pingfa and Zhang, Jianwei",
        "bdsk-url-1": "https://doi.org/10.1109/ICRA48891.2023.10160491",
        "booktitle": "2023 IEEE International Conference on Robotics and Automation (ICRA)",
        "doi": "10.1109/ICRA48891.2023.10160491",
        "keywords": "Training;Shape;Stacking;Grasping;Reinforcement learning;Feature extraction;Trajectory, 1, non-self-supervised",
        "month": "May",
        "pages": "3860-3866",
        "series": "ICRA",
        "title": "Reinforcement Learning Based Pushing and Grasping Objects from Ungraspable Poses",
        "type": "inproceedings",
        "year": "2023"
    },
    "8593986": {
        "abstract": "Skilled robotic manipulation benefits from complex synergies between non-prehensile (e.g. pushing) and prehensile (e.g. grasping) actions: pushing can help rearrange cluttered objects to make space for arms and fingers; likewise, grasping can help displace objects to make pushing movements more precise and collision-free. In this work, we demonstrate that it is possible to discover and learn these synergies from scratch through model-free deep reinforcement learning. Our method involves training two fully convolutional networks that map from visual observations to actions: one infers the utility of pushes for a dense pixel-wise sampling of end-effector orientations and locations, while the other does the same for grasping. Both networks are trained jointly in a Q-learning framework and are entirely self-supervised by trial and error, where rewards are provided from successful grasps. In this way, our policy learns pushing motions that enable future grasps, while learning grasps that can leverage past pushes. During picking experiments in both simulation and real-world scenarios, we find that our system quickly learns complex behaviors even amid challenging cases of tightly packed clutter, and achieves better grasping success rates and picking efficiencies than baseline alternatives after a few hours of training. We further demonstrate that our method is capable of generalizing to novel objects. Qualitative results (videos), code, pre-trained models, and simulation environments are available at http://vpg.cs.princeton.edu/",
        "author": "Zeng, Andy and Song, Shuran and Welker, Stefan and Lee, Johnny and Rodriguez, Alberto and Funkhouser, Thomas",
        "bdsk-url-1": "https://doi.org/10.1109/IROS.2018.8593986",
        "booktitle": "2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
        "doi": "10.1109/IROS.2018.8593986",
        "issn": "2153-0866",
        "keywords": "Grasping;Training;Three-dimensional displays;Reinforcement learning;Planning;Manipulators, self-supervised",
        "month": "Oct",
        "pages": "4238-4245",
        "series": "IROS",
        "title": "Learning Synergies Between Pushing and Grasping with Self-Supervised Deep Reinforcement Learning",
        "type": "inproceedings",
        "year": "2018"
    },
    "8976257": {
        "abstract": "We study an emerging problem named ``grasping the invisible'' in robotic manipulation, in which a robot is tasked to grasp an initially invisible target object via a sequence of pushing and grasping actions. In this problem, pushes are needed to search for the target and rearrange cluttered objects around it to enable effective grasps. We propose to solve the problem by formulating a deep learning approach in a critic-policy format. The target-oriented motion critic, which maps both visual observations and target information to the expected future rewards of pushing and grasping motion primitives, is learned via deep Q-learning. We divide the problem into two subtasks, and two policies are proposed to tackle each of them, by combining the critic predictions and relevant domain knowledge. A Bayesian-based policy accounting for past action experience performs pushing to search for the target; once the target is found, a classifier-based policy coordinates target-oriented pushing and grasping to grasp the target in clutter. The motion critic and the classifier are trained in a self-supervised manner through robot-environment interactions. Our system achieves a 93% and 87% task success rate on each of the two subtasks in simulation and an 85% task success rate in real robot experiments on the whole problem, which outperforms several baselines by large margins. Supplementary material is available at https://sites.google.com/umn.edu/grasping-invisible.",
        "author": "Yang, Yang and Liang, Hengyue and Choi, Changhyun",
        "bdsk-url-1": "https://doi.org/10.1109/LRA.2020.2970622",
        "doi": "10.1109/LRA.2020.2970622",
        "issn": "2377-3766",
        "journal": "IEEE Robotics and Automation Letters",
        "keywords": "Grasping;Clutter;Robot kinematics;Task analysis;Deep learning;Visualization;Dexterous manipulation;deep learning in robotics and automation;computer vision for automation, self-supervised",
        "month": "April",
        "number": "2",
        "pages": "2232-2239",
        "series": "LRA",
        "title": "A Deep Learning Approach to Grasping the Invisible",
        "type": "article",
        "volume": "5",
        "year": "2020"
    },
    "9536651": {
        "abstract": "Directly grasping the tightly stacked objects may cause collisions and result in failures, degenerating the functionality of robotic arms. Inspired by the observation that first pushing objects to a state of mutual separation and then grasping them individually can effectively increase the success rate, we devise a novel deep Q-learning framework to achieve collaborative pushing and grasping. Specifically, an efficient non-maximum suppression policy (policyNMS) is proposed to dynamically evaluate pushing and grasping actions by enforcing a suppression constraint on unreasonable actions. Moreover, a novel data-driven pushing reward network called PR-Net is designed to effectively assess the degree of separation or aggregation between objects. To benchmark the proposed method, we establish a dataset containing common household items dataset (CHID) in both simulation and real scenarios. Although trained using simulation data only, experiment results validate that our method generalizes well to real scenarios and achieves a 97% grasp success rate at a fast speed for object separation in the real-world environment.",
        "author": "Yang, Yuxiang and Ni, Zhihao and Gao, Mingyu and Zhang, Jing and Tao, Dacheng",
        "bdsk-url-1": "https://doi.org/10.1109/JAS.2021.1004255",
        "doi": "10.1109/JAS.2021.1004255",
        "issn": "2329-9274",
        "journal": "IEEE/CAA Journal of Automatica Sinica",
        "keywords": "Grasping,Reinforcement learning,Manipulators,Convolutional, neural network, deep Q-learning (DQN),reward function,robotic grasping;robotic pushing, self-supervised",
        "month": "January",
        "number": "1",
        "pages": "135-145",
        "series": "JAS",
        "title": "Collaborative Pushing and Grasping of Tightly Stacked Objects via Deep Reinforcement Learning",
        "type": "article",
        "volume": "9",
        "year": "2022"
    },
    "9561828": {
        "abstract": "Robots must reason about pushing and grasping in order to engage in flexible manipulation in cluttered environments. Earlier works on learning pushing and grasping only consider each operation in isolation or are limited to top-down grasping and bin-picking. We train a robot to learn joint planar pushing and 6-degree-of-freedom (6-DoF) grasping policies by self-supervision. Two separate deep neural networks are trained to map from 3D visual observations to actions with a Q-learning framework. With collaborative pushes and expanded grasping action space, our system can deal with cluttered scenes with a wide variety of objects (e.g. grasping a plate from the side after pushing away surrounding obstacles). We compare our system to the state-of-the-art baseline model VPG [1] in simulation and outperform it with 10% higher action efficiency and 20% higher grasp success rate. We then demonstrate our system on a KUKA LBR iiwa arm with a Robotiq 3-finger gripper.",
        "author": "Tang, Bingjie and Corsaro, Matthew and Konidaris, George and Nikolaidis, Stefanos and Tellex, Stefanie",
        "bdsk-url-1": "https://doi.org/10.1109/ICRA48506.2021.9561828",
        "booktitle": "2021 IEEE International Conference on Robotics and Automation (ICRA)",
        "doi": "10.1109/ICRA48506.2021.9561828",
        "issn": "2577-087X",
        "keywords": "Deep learning,Visualization,Three-dimensional displays,Automation,Conferences,Collaboration,Grasping,self-supervised",
        "month": "May",
        "pages": "6177-6184",
        "series": "ICRA",
        "title": "Learning Collaborative Pushing and Grasping Policies in Dense Clutter",
        "type": "inproceedings",
        "year": "2021"
    },
    "9591286": {
        "abstract": "This letter considers the problem of retrieving an object from many tightly packed objects using a combination of robotic pushing and grasping actions. Object retrieval in dense clutter is an important skill for robots to operate in households and everyday environments effectively. The proposed solution, Visual Foresight Tree (VFT), intelligently rearranges the clutter surrounding a target object so that it can be grasped easily. Rearrangement with nested nonprehensile actions is challenging as it requires predicting complex object interactions in a combinatorially large configuration space of multiple objects. We first show that a deep neural network can be trained to accurately predict the poses of the packed objects when the robot pushes one of them. The predictive network provides visual foresight and is used in a tree search as a state transition function in the space of scene images. The tree search returns a sequence of consecutive push actions yielding the best arrangement of the clutter for grasping the target object. Experiments in simulation and using a real robot and objects show that the proposed approach outperforms model-free techniques as well as model-based myopic methods both in terms of success rates and the number of executed actions, on several challenging tasks. A video introducing VFT, with robot experiments, is accessible at https://youtu.be/7cL-hmgvyec. The full source code is available at https://github.com/arc-l/vft.",
        "author": "Huang, Baichuan and Han, Shuai D. and Yu, Jingjin and Boularias, Abdeslam",
        "bdsk-url-1": "https://doi.org/10.1109/LRA.2021.3123373",
        "doi": "10.1109/LRA.2021.3123373",
        "issn": "2377-3766",
        "journal": "IEEE Robotics and Automation Letters",
        "keywords": "Robots,Clutter,Grasping,Grippers,Visualization,Task analysis,Search problems,Deep learning in grasping and manipulation,learning from experience;visual learning, non-self-supervised",
        "month": "Jan",
        "number": "1",
        "pages": "231-238",
        "series": "LRA",
        "title": "Visual Foresight Trees for Object Retrieval From Clutter With Nonprehensile Rearrangement",
        "type": "article",
        "volume": "7",
        "year": "2022"
    },
    "9811645": {
        "abstract": "Efficient robotic manipulation of objects for sorting and searching often rely upon how well the objects are perceived and the available grasp poses. The challenge arises when the objects are irregular, have similar visual features (e.g., textureless objects) and the scene is densely cluttered. In such cases, non-prehensile manipulation (e.g., pushing) can facilitate grasping or searching by improving object perception and singulating the objects from the clutter via physical interaction. The current robotics literature in interactive segmentation focuses solely on isolated cases, where the central aim is on searching or singulating a single target object, or segmenting sparsely cluttered scenes, mainly through matching visual futures in successive scenes before and after the robotic interaction. On the other hand, in this paper, we introduce the first interactive segmentation model in the literature that can autonomously enhance the instance segmentation of such challenging scenes as a whole via optimising a Q-value function that predicts appropriate pushing actions for singulation. We achieved this by training a deep reinforcement learning model with reward signals generated by a Mask-RCNN trained solely on depth images. We evaluated our model in experiments by comparing its success on segmentation quality with a heuristic baseline, as well as the state-of-the-art Visual Pushing and Grasping (VPG) model [1]. Our model significantly outperformed both baselines in all benchmark scenarios. Furthermore, decreasing the segmentation error inherently enabled the autonomous singulation of the scene as a whole. Our evaluation experiments also serve as a benchmark for interactive segmentation research.",
        "author": "Serhan, Baris and Pandya, Harit and Kucukyilmaz, Ayse and Neumann, Gerhard",
        "bdsk-file-1": "YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAYV1gyMDI0MDUxNS0yMDQwMTNAMngucG5nTxEBegAAAAABegACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAA4WnW6EJEAAH/////GFdYMjAyNDA1MTUtMjA0MDEzQDJ4LnBuZwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP/////ias4vAAAAAAAAAAAAAQACAAAKIGN1AAAAAAAAAAAAAAAAAAlEb3dubG9hZHMAAAIANS86VXNlcnM6b3Vyb3VjaGVuZzpEb3dubG9hZHM6V1gyMDI0MDUxNS0yMDQwMTNAMngucG5nAAAOADIAGABXAFgAMgAwADIANAAwADUAMQA1AC0AMgAwADQAMAAxADMAQAAyAHgALgBwAG4AZwAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASADNVc2Vycy9vdXJvdWNoZW5nL0Rvd25sb2Fkcy9XWDIwMjQwNTE1LTIwNDAxM0AyeC5wbmcAABMAAS8AABUAAgAR//8AAAAIAA0AGgAkAD8AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABvQ",
        "booktitle": "2022 International Conference on Robotics and Automation (ICRA)",
        "doi": "10.1109/ICRA46639.2022.9811645",
        "keywords": "Training,Visualization,Image segmentation,Q-learning,Protocols,Grasping,Benchmark testing, self-supervised",
        "month": "May",
        "pages": "1513-1519",
        "series": "ICRA",
        "title": "Push-to-See: Learning Non-Prehensile Manipulation to Enhance Instance Segmentation via Deep Q-Learning",
        "type": "inproceedings",
        "year": "2022"
    },
    "9880518": {
        "abstract": "We explore a method for grasping novel target objects through push-grasping synergy in a cluttered environment without using object detection and segmentation algorithms. The target information is represented by a color heightmap of the target object. The agent needs to implicitly find the corresponding object in the cluttered scene according to the target information and gives action instructions. We propose an action space decoupling framework to efficiently address the problem of policy learning in large state spaces, which predicts the grasping position and the grasping angle separately. Our system needs to learn two policy networks: position net and angle net. The position net infers the appropriate grasping position and the starting position of the pushing motion, and the angle net predicts the grasping angle based on the grasping position. In addition, the angle net is also a coordinator to determine whether the current state should perform the grasping action or the pushing action. A series of experiments show that the proposed method can quickly learn complex push-grasping coordinated policies in a cluttered environment and the task success rate and the grasping efficiency are greatly improved compared to the baseline methods and it has good generalization ability for novel objects. Furthermore, our method can be transferred to the real world without fine-tuning.",
        "author": "Li, Enbo and Feng, Haibo and Zhang, Songyuan and Fu, Yili",
        "bdsk-url-1": "https://doi.org/10.1109/LRA.2022.3204822",
        "doi": "10.1109/LRA.2022.3204822",
        "issn": "2377-3766",
        "journal": "IEEE Robotics and Automation Letters",
        "keywords": "Grasping;Image color analysis;Task analysis;Object detection;Training;Grippers;Cameras;Deep learning in grasping and manipulation;grasping;reinforcement learning, self-supervised",
        "month": "Oct",
        "number": "4",
        "pages": "11966-11973",
        "series": "LRA",
        "title": "Learning Target-Oriented Push-Grasping Synergy in Clutter With Action Space Decoupling",
        "type": "article",
        "volume": "7",
        "year": "2022"
    },
    "9981499": {
        "abstract": "Grasping in dense clutter is a fundamental skill for autonomous robots. However, the crowdedness and oc-clusions in the cluttered scenario cause significant difficul-ties to generate valid grasp poses without collisions, which results in low efficiency and high failure rates. To address these, we present a generic framework called GE-Grasp for robotic motion planning in dense clutter, where we leverage diverse action primitives for occluded object removal and present the generator-evaluator architecture to avoid spatial collisions. Therefore, our GE-Grasp is capable of grasping objects in dense clutter efficiently with promising success rates. Specifically, we define three action primitives: target-oriented grasping for target capturing, pushing, and nontarget-oriented grasping to reduce the crowdedness and occlusions. The gen-erators effectively provide various action candidates referring to the spatial information. Meanwhile, the evaluators assess the selected action primitive candidates, where the optimal action is implemented by the robot. Extensive experiments in simulated and real-world environments show that our approach outperforms the state-of-the-art methods of grasping in clutter with respect to motion efficiency and success rates. Moreover, we achieve comparable performance in the real world as that in the simulation environment, which indicates the strong gen-eralization ability of our GE-Grasp. Supplementary material is available at: https://github.com/CaptainWuDaoKou/GE-Grasp.",
        "author": "Liu, Zhan and Wang, Ziwei and Huang, Sichao and Zhou, Jie and Lu, Jiwen",
        "bdsk-url-1": "https://doi.org/10.1109/IROS47612.2022.9981499",
        "booktitle": "2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
        "doi": "10.1109/IROS47612.2022.9981499",
        "issn": "2153-0866",
        "keywords": "Robot motion,Grasping,Planning,Clutter,Intelligent robots,Autonomous robots, non-self-supervised",
        "month": "Oct",
        "pages": "1388-1395",
        "series": "IROS",
        "title": "GE-Grasp: Efficient Target-Oriented Grasping in Dense Clutter",
        "type": "inproceedings",
        "year": "2022"
    },
    "Lu2022PickingOT": {
        "abstract": "``Picking out the impurities'' is a typical scenario in production line which is both time consuming and laborious. In this article, we propose a target-oriented robotic push-grasping system which is able to actively discover and pick the impurities in dense environments with the synergies between pushing and grasping actions. First, we propose an attention module, which includes target saliency detection and density-based occluded-region inference. Without the necessity of expensive labeling of semantic segmentation, our attention module can quickly locate the targets in the view or predict the candidate regions where the targets are most likely to be occluded. Second, we propose a push--grasp synergy framework to sequentially select proper actions in different situations until all targets are picked out. Moreover, we introduce an active pushing mechanism based on a novel metric, namely Target-Centric Dispersion Degree (TCDD) for better grasping. TCDD describes whether the targets are isolated from the surrounding objects. With this metric, the robot becomes more focused on the actions around the targets and push irrelevant objects away. Experimental results on both simulated environment and real-world environment show that our proposed system outperforms several baseline approaches,which also has the capability to be generalized to new scenarios.",
        "author": "Ning Lu and Yinghao Cai and Tao Lu and Xiaoge Cao and Weiyan Guo and Shuo Wang",
        "bdsk-url-1": "https://api.semanticscholar.org/CorpusID:247726598",
        "bdsk-url-2": "https://doi.org/10.1017/S0263574722000297",
        "doi": "https://doi.org/10.1017/S0263574722000297",
        "journal": "Robotica",
        "keywords": "non-self-supervised",
        "pages": "470 - 485",
        "title": "Picking out the Impurities: Attention-based Push-Grasping in Dense Clutter",
        "type": "article",
        "url": "https://api.semanticscholar.org/CorpusID:247726598",
        "volume": "41",
        "year": "2023"
    }
}});